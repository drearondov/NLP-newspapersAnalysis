{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "## Introduction\n",
    "\n",
    "After cleaning the data we are going to take a look a it. And since we want to know how the information changes across time, we will be looking at tweets from different weeks.\n",
    "\n",
    "1. **Most common words:** Find them and create word clouds. See if anything needs to be removed.\n",
    "2. **Size of vocabulary:** Look at the number of unique words used\n",
    "3. **Engagement metrics across time:** A much insightfull look into the stats obtained during data cleaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "from dash import Dash, dcc, html, Input, Output\n",
    "from dotenv import load_dotenv\n",
    "from itertools import product\n",
    "from jupyter_dash import JupyterDash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "BASE_DIR = os.environ.get(\"BASE_DIR\")\n",
    "BEARER_TOKEN = os.environ.get(\"BEARER_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 300)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "pd.set_option(\"display.precision\", 2)\n",
    "pd.set_option(\"display.float_format\", \"{:,.2f}\".format)\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "pio.kaleido.scope.default_scale = 2\n",
    "\n",
    "gruvbox_colors = [\n",
    "    \"#fabd2f\",\n",
    "    \"#b8bb26\",\n",
    "    \"#458588\",\n",
    "    \"#fe8019\",\n",
    "    \"#b16286\",\n",
    "    \"#fb4943\",\n",
    "    \"#689d6a\",\n",
    "    \"#d79921\",\n",
    "    \"#98971a\",\n",
    "    \"#83a598\",\n",
    "    \"#d65d0e\",\n",
    "    \"#d3869b\",\n",
    "    \"#cc241d\",\n",
    "    \"#8ec07c\",\n",
    "    \"#b57614\",\n",
    "    \"#79740e\",\n",
    "    \"#076678\",\n",
    "    \"#af3a03\",\n",
    "    \"#8f3f71\",\n",
    "    \"#9d0006\",\n",
    "    \"#4d7b58\",\n",
    "    \"#fbf1c7\",\n",
    "    \"#928374\",\n",
    "    \"#282828\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STAMPS = [(2022, 35), (2022, 40), (2022, 45), (2022, 50), (2023, 3)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "There are three documents that I want to load. The corpus frame, document term matrix and clean data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_feather(\n",
    "    f\"{BASE_DIR}/data/processed/corpus-{TIME_STAMPS[0]}-{TIME_STAMPS[-1]}.feather\"\n",
    ")\n",
    "dtm = pd.read_feather(\n",
    "    f\"{BASE_DIR}/data/processed/dtm-{TIME_STAMPS[0]}-{TIME_STAMPS[-1]}.feather\"\n",
    ")\n",
    "data_dtm = pd.read_feather(\n",
    "    f\"{BASE_DIR}/data/processed/data-dtm-{TIME_STAMPS[0]}-{TIME_STAMPS[-1]}.feather\"\n",
    ")\n",
    "stats_data = pd.read_feather(\n",
    "    f\"{BASE_DIR}/data/processed/stats_data-{TIME_STAMPS[0]}-{TIME_STAMPS[-1]}.feather\"\n",
    ")\n",
    "top30_df = pd.read_feather(\n",
    "    f\"{BASE_DIR}/data/processed/top30_df-{TIME_STAMPS[0]}-{TIME_STAMPS[-1]}.feather\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm.set_index(\"index\", inplace=True)\n",
    "data_dtm.set_index(\"index\", inplace=True)\n",
    "corpus.set_index(\"index\", inplace=True)\n",
    "stats_data.set_index(\"index\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newspapers = corpus[\"newspaper\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_weeks = corpus[\"created_at\"].dt.isocalendar()[[\"year\", \"week\"]]\n",
    "year_weeks.drop_duplicates(inplace=True)\n",
    "year_weeks = year_weeks.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_newspaper = pd.DataFrame(index=dtm.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[\"year\"] = corpus[\"created_at\"].dt.isocalendar().year\n",
    "corpus[\"week\"] = corpus[\"created_at\"].dt.isocalendar().week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_newspaper = pd.DataFrame(index=dtm.columns)\n",
    "\n",
    "for year_week, newspaper in product(year_weeks, newspapers):\n",
    "    data_ids = corpus.loc[\n",
    "        (corpus[\"newspaper\"] == newspaper)\n",
    "        & (corpus[\"year\"] == year_week[0])\n",
    "        & (corpus[\"week\"] == year_week[1]),\n",
    "        [\"id\"],\n",
    "    ]\n",
    "    filtered_data = dtm.filter(items=data_ids[\"id\"], axis=0)\n",
    "    dtm_newspaper[f\"{newspaper}-{year_week[0]}_{year_week[1]}\"] = filtered_data.sum(\n",
    "        axis=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_dict = {}\n",
    "\n",
    "for newspaper in dtm_newspaper.columns:\n",
    "    top = dtm_newspaper[newspaper].sort_values(ascending=False).head(30)\n",
    "    top30_dict[newspaper] = list(zip(top.index, top.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_df = pd.DataFrame.from_records(top30_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_df = top30_df.melt(\n",
    "    value_vars=top30_df.columns, var_name=\"newspaper_date\", value_name=\"word_count\"\n",
    ")\n",
    "\n",
    "top30_df[[\"newspaper\", \"year_week\"]] = top30_df[\"newspaper_date\"].str.split(\n",
    "    r\"-\", expand=True\n",
    ")\n",
    "top30_df[[\"year\", \"week\"]] = top30_df[\"year_week\"].str.split(r\"_\", expand=True)\n",
    "top30_df[[\"word\", \"count\"]] = pd.DataFrame(\n",
    "    top30_df[\"word_count\"].to_list(), index=top30_df.index\n",
    ")\n",
    "\n",
    "top30_df.drop([\"word_count\", \"newspaper_date\", \"year_week\"], axis=1, inplace=True)\n",
    "\n",
    "top30_df[\"year\"] = pd.to_numeric(top30_df[\"year\"])\n",
    "top30_df[\"week\"] = pd.to_numeric(top30_df[\"week\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_df.to_feather(\n",
    "    f\"{BASE_DIR}/data/processed/top30_df-{TIME_STAMPS[0]}-{TIME_STAMPS[-1]}.feather\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_df[\"hot_topics\"] = top30_df[\"word\"].map(\n",
    "    {\n",
    "        \"castillo\": \"castillo\",\n",
    "        \"pedro\": \"castillo\",\n",
    "        \"dina\": \"boluarte\",\n",
    "        \"boluarte\": \"boluarte\",\n",
    "        \"perú\": \"país\",\n",
    "        \"país\": \"país\",\n",
    "        \"congreso\": \"congreso\",\n",
    "        \"covid\": \"covid\",\n",
    "        \"protestas\": \"protestas\",\n",
    "        \"manifestaciones\": \"protestas\",\n",
    "    }\n",
    ")\n",
    "top30_df[\"hot_topics\"].fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    f\"{BASE_DIR}/data/processed/top_30-{TIME_STAMPS[0]}-{TIME_STAMPS[-1]}.json\", \"w\"\n",
    ") as file:\n",
    "    json.dump(top30_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    top30_df,\n",
    "    x=\"word\",\n",
    "    y=\"count\",\n",
    "    facet_row=\"newspaper\",\n",
    "    facet_col=\"week\",\n",
    "    color=\"hot_topics\",\n",
    "    color_discrete_sequence=gruvbox_colors,\n",
    "    title=\"Top 30 words per newspaper per week\",\n",
    "    height=3200,\n",
    "    width=3200,\n",
    ")\n",
    "\n",
    "fig.for_each_annotation(lambda a: a.update(text=f\"{a.text.split('=')[-1]}\"))\n",
    "fig.update_xaxes(matches=None, showticklabels=True, categoryorder=\"total descending\")\n",
    "fig.update_yaxes(matches=None, showticklabels=True)\n",
    "\n",
    "fig.write_html(f\"{BASE_DIR}/reports/top30_bar-{TIME_STAMPS[0]}-{TIME_STAMPS[-1]}.html\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_list = []\n",
    "\n",
    "# Identify the non-zero items in the document-term matrix\n",
    "for newspaper in dtm_newspaper.columns:\n",
    "    uniques = dtm_newspaper[newspaper].to_numpy().nonzero()[0].size\n",
    "    unique_list.append(uniques)\n",
    "\n",
    "# Create a new datafra,e that contains this unique word count\n",
    "data_words = pd.DataFrame(\n",
    "    list(zip(dtm_newspaper.columns, unique_list)), columns=[\"newspaper\", \"unique_words\"]\n",
    ")\n",
    "data_words.set_index(\"newspaper\", inplace=True)\n",
    "data_words.sort_values(by=\"unique_words\", ascending=False)\n",
    "data_words.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words[[\"newspaper\", \"year_week\"]] = data_words[\"newspaper\"].str.split(\n",
    "    r\"-\", expand=True\n",
    ")\n",
    "data_words[[\"year\", \"week\"]] = data_words[\"year_week\"].str.split(r\"_\", expand=True)\n",
    "\n",
    "data_words.drop([\"year_week\"], axis=1, inplace=True)\n",
    "\n",
    "data_words[\"year\"] = pd.to_numeric(data_words[\"year\"])\n",
    "data_words[\"week\"] = pd.to_numeric(data_words[\"week\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of unique words might be linked to the number of tweets, I will add a column with the number of tweets for each newspaper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_number = pd.DataFrame(\n",
    "    corpus.groupby(by=[\"newspaper\", \"year\", \"week\"]).count()[\"id\"]\n",
    ")\n",
    "tweet_number.rename(columns={\"id\": \"tweet_number\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_number.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_number.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words = data_words.merge(tweet_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words[\"word_tweet_ratio\"] = data_words[\"unique_words\"] / data_words[\"tweet_number\"]\n",
    "data_words.sort_values(by=\"word_tweet_ratio\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words.to_csv(\n",
    "    f\"{BASE_DIR}/reports/tables/words_tweets-{TIME_STAMPS[0]}-{TIME_STAMPS[-1]}.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    data_words,\n",
    "    \"unique_words\",\n",
    "    \"tweet_number\",\n",
    "    facet_col=\"week\",\n",
    "    color=\"newspaper\",\n",
    "    color_discrete_sequence=gruvbox_colors,\n",
    "    title=\"Unique words per newspaper\",\n",
    "    width=2400,\n",
    "    height=600,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words = pd.read_csv(\n",
    "    f\"{BASE_DIR}/reports/tables/words_tweets-{TIME_STAMPS[0]}-{TIME_STAMPS[-1]}.csv\",\n",
    "    index_col=0,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engagement\n",
    "\n",
    "Now I will be looking into engagement metrics as a whole per newspaper and how it changes, as well as the relationship with the most used words as well as the ammount of vocabulary expressed in the most used words.\n",
    "\n",
    "### Raw stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_data[\"year\"] = stats_data[\"created_at\"].dt.isocalendar().year\n",
    "stats_data[\"week\"] = stats_data[\"created_at\"].dt.isocalendar().week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_summary = (\n",
    "    stats_data[\n",
    "        [\n",
    "            \"newspaper\",\n",
    "            \"retweet_count\",\n",
    "            \"reply_count\",\n",
    "            \"like_count\",\n",
    "            \"quote_count\",\n",
    "            \"year\",\n",
    "            \"week\",\n",
    "        ]\n",
    "    ]\n",
    "    .groupby(by=[\"newspaper\", \"year\", \"week\"])\n",
    "    .agg(func=[\"count\", \"min\", \"mean\", \"std\", \"max\", \"sum\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_summary[(\"retweet_count\", \"ratio\")] = (\n",
    "    stats_summary[(\"retweet_count\", \"sum\")] / stats_summary[(\"retweet_count\", \"count\")]\n",
    ")\n",
    "stats_summary[(\"reply_count\", \"ratio\")] = (\n",
    "    stats_summary[(\"reply_count\", \"sum\")] / stats_summary[(\"reply_count\", \"count\")]\n",
    ")\n",
    "stats_summary[(\"like_count\", \"ratio\")] = (\n",
    "    stats_summary[(\"like_count\", \"sum\")] / stats_summary[(\"like_count\", \"count\")]\n",
    ")\n",
    "stats_summary[(\"quote_count\", \"ratio\")] = (\n",
    "    stats_summary[(\"quote_count\", \"sum\")] / stats_summary[(\"quote_count\", \"count\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_summary = stats_summary.stack()\n",
    "stats_summary = stats_summary.melt(var_name=\"metric\", ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_summary = stats_summary.reset_index()\n",
    "stats_summary.rename({\"level_3\": \"stat\"}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_summary[\"year_week\"] = (\n",
    "    stats_summary[\"year\"].astype(\"str\") + \"w\" + stats_summary[\"week\"].astype(\"str\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_summary.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    stats_summary,\n",
    "    x=\"year_week\",\n",
    "    y=\"value\",\n",
    "    color=\"newspaper\",\n",
    "    facet_row=\"metric\",\n",
    "    facet_row_spacing=0.08,\n",
    "    facet_col=\"stat\",\n",
    "    color_discrete_sequence=gruvbox_colors,\n",
    "    title=\"Raw engagement stats per newspaper\",\n",
    "    width=2100,\n",
    "    height=1200,\n",
    ")\n",
    "\n",
    "fig.for_each_annotation(lambda a: a.update(text=f\"{a.text.split('=')[-1]}\"))\n",
    "fig.update_xaxes(showticklabels=True, tickangle=-45)\n",
    "fig.update_yaxes(matches=None, showticklabels=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_summary.to_csv(\n",
    "    f\"{BASE_DIR}/reports/tables/raw_stats_summary-{TIME_STAMPS[0]}-{TIME_STAMPS[-1]}.csv\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats for most used words\n",
    "\n",
    "With this bit I want to find out which are the words that drive the most engagement for the newspaper durin the time period selected. For that I'll be building a small dashboard application using [Plotly Dash](https://dash.plotly.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dtm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dtm[\"year\"] = data_dtm[\"created_at\"].dt.isocalendar().year\n",
    "data_dtm[\"week\"] = data_dtm[\"created_at\"].dt.isocalendar().week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_ids = pd.merge(\n",
    "    data_dtm, top30_df, how=\"right\", on=[\"newspaper\", \"year\", \"week\", \"word\"]\n",
    ")\n",
    "top30_ids.dropna(subset=[\"id\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_stats = pd.merge(\n",
    "    top30_ids,\n",
    "    stats_data,\n",
    "    how=\"left\",\n",
    "    on=[\"id\", \"created_at\", \"newspaper\", \"year\", \"week\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_stats.reset_index().to_feather(\n",
    "    f\"{BASE_DIR}/data/processed/top30-stats-{TIME_STAMPS[0]}-{TIME_STAMPS[-1]}.feather\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_stats_summary = (\n",
    "    top30_stats[\n",
    "        [\n",
    "            \"newspaper\",\n",
    "            \"retweet_count\",\n",
    "            \"reply_count\",\n",
    "            \"like_count\",\n",
    "            \"quote_count\",\n",
    "            \"word\",\n",
    "            \"year\",\n",
    "            \"week\",\n",
    "        ]\n",
    "    ]\n",
    "    .groupby(by=[\"newspaper\", \"year\", \"week\", \"word\"])\n",
    "    .agg(func=[\"count\", \"min\", \"mean\", \"std\", \"max\", \"sum\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_stats_summary[(\"retweet_count\", \"ratio\")] = (\n",
    "    top30_stats_summary[(\"retweet_count\", \"sum\")]\n",
    "    / top30_stats_summary[(\"retweet_count\", \"count\")]\n",
    ")\n",
    "top30_stats_summary[(\"reply_count\", \"ratio\")] = (\n",
    "    top30_stats_summary[(\"reply_count\", \"sum\")]\n",
    "    / top30_stats_summary[(\"reply_count\", \"count\")]\n",
    ")\n",
    "top30_stats_summary[(\"like_count\", \"ratio\")] = (\n",
    "    top30_stats_summary[(\"like_count\", \"sum\")]\n",
    "    / top30_stats_summary[(\"like_count\", \"count\")]\n",
    ")\n",
    "top30_stats_summary[(\"quote_count\", \"ratio\")] = (\n",
    "    top30_stats_summary[(\"quote_count\", \"sum\")]\n",
    "    / top30_stats_summary[(\"quote_count\", \"count\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_stats_summary = top30_stats_summary.stack()\n",
    "top30_stats_summary = top30_stats_summary.melt(var_name=\"metric\", ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_stats_summary = top30_stats_summary.reset_index()\n",
    "top30_stats_summary.rename({\"level_4\": \"stat\"}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_stats_summary[\"year_week\"] = (\n",
    "    top30_stats_summary[\"year\"].astype(\"str\")\n",
    "    + \"w\"\n",
    "    + top30_stats_summary[\"week\"].astype(\"str\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_stats_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_stats_summary[\"hot_topics\"] = top30_stats_summary[\"word\"].map(\n",
    "    {\n",
    "        \"castillo\": \"castillo\",\n",
    "        \"pedro\": \"castillo\",\n",
    "        \"dina\": \"boluarte\",\n",
    "        \"boluarte\": \"boluarte\",\n",
    "        \"perú\": \"país\",\n",
    "        \"país\": \"país\",\n",
    "        \"congreso\": \"congreso\",\n",
    "        \"covid\": \"covid\",\n",
    "        \"protestas\": \"protestas\",\n",
    "        \"manifestaciones\": \"protestas\",\n",
    "    }\n",
    ")\n",
    "top30_stats_summary[\"hot_topics\"].fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_stats_summary.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_stats_summary.to_csv(\n",
    "    f\"{BASE_DIR}/reports/tables/top30_stats_summary-{TIME_STAMPS[0]}-{TIME_STAMPS[-1]}.csv\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_stats_summary = pd.read_csv(\n",
    "    f\"{BASE_DIR}/reports/tables/top30_stats_summary-{TIME_STAMPS[0]}-{TIME_STAMPS[-1]}.csv\",\n",
    "    index_col=0,\n",
    ")\n",
    "top30_stats_summary[\"hot_topics\"].fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_app = JupyterDash(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_app.layout = html.Div(\n",
    "    children=[\n",
    "        html.H1(\n",
    "            \"Top 30 words engagement stats per newspaper\",\n",
    "            style={\"font-family\": \"Open Sans\", \"color\": \"#2a3f5f\"},\n",
    "        ),\n",
    "        html.Br(),\n",
    "        html.Div(\n",
    "            [\n",
    "                html.Div(\n",
    "                    [\n",
    "                        html.Label(\n",
    "                            \"Stat\",\n",
    "                            style={\"font-family\": \"Open Sans\", \"color\": \"#2a3f5f\"},\n",
    "                        ),\n",
    "                        dcc.RadioItems(\n",
    "                            top30_stats_summary[\"stat\"].unique(),\n",
    "                            \"ratio\",\n",
    "                            id=\"stat\",\n",
    "                            inline=True,\n",
    "                            style={\"font-family\": \"Open Sans\", \"color\": \"#2a3f5f\"},\n",
    "                        ),\n",
    "                    ]\n",
    "                ),\n",
    "                html.Div(\n",
    "                    [\n",
    "                        html.Label(\n",
    "                            \"Metric\",\n",
    "                            style={\"font-family\": \"Open Sans\", \"color\": \"#2a3f5f\"},\n",
    "                        ),\n",
    "                        dcc.RadioItems(\n",
    "                            top30_stats_summary[\"metric\"].unique(),\n",
    "                            \"like_count\",\n",
    "                            id=\"metric\",\n",
    "                            inline=True,\n",
    "                            style={\"font-family\": \"Open Sans\", \"color\": \"#2a3f5f\"},\n",
    "                        ),\n",
    "                    ]\n",
    "                ),\n",
    "            ],\n",
    "            style={\"display\": \"flex\", \"justify-content\": \"space-around\"},\n",
    "        ),\n",
    "        html.Br(),\n",
    "        dcc.Graph(id=\"stats_graph\"),\n",
    "    ],\n",
    "    style={\"height\": \"3600px\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@top30_app.callback(\n",
    "    Output(\"stats_graph\", \"figure\"), Input(\"stat\", \"value\"), Input(\"metric\", \"value\")\n",
    ")\n",
    "def update_figure(selected_stat, selected_metric):\n",
    "    filtered_data = top30_stats_summary.loc[\n",
    "        (top30_stats_summary[\"stat\"] == selected_stat)\n",
    "        & (top30_stats_summary[\"metric\"] == selected_metric)\n",
    "    ]\n",
    "\n",
    "    fig = px.bar(\n",
    "        filtered_data,\n",
    "        x=\"word\",\n",
    "        y=\"value\",\n",
    "        facet_row=\"newspaper\",\n",
    "        facet_col=\"year_week\",\n",
    "        color=\"hot_topics\",\n",
    "        color_discrete_sequence=gruvbox_colors,\n",
    "        height=3200,\n",
    "        width=3200,\n",
    "    )\n",
    "\n",
    "    fig.for_each_annotation(lambda a: a.update(text=f\"{a.text.split('=')[-1]}\"))\n",
    "    fig.update_xaxes(\n",
    "        matches=None, showticklabels=True, categoryorder=\"total descending\"\n",
    "    )\n",
    "    fig.update_yaxes(matches=None, showticklabels=True)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    top30_app.run_server(mode=\"inline\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-newspapersAnalysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "aef0c461847f6a88eedb677fa00493df291ae4b245cdf057d207d54f564bf672"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
