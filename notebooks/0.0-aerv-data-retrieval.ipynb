{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook goes through the steps taken with the data collected in order to get cleaned organized data in two standard text formats. The notebook will contain the nexts steps.\n",
    "\n",
    "1. **Cleaning the data -** I will use text pre-procesing techniques to get the dta into shape.\n",
    "2. **Organizing the data -** I'l organize the data into a way that is easy to input into other algoithms\n",
    "\n",
    "The output of this notebook will be clean, organized data in two standard text formats:\n",
    "\n",
    "1. **Corpus** - a collection of texts\n",
    "2. **Document-Term Matrix** - words counts in matrix format\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "My goal is to look look a the latest headlines of the main newspapers in Perú and note simmilarities and differences."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "import requests\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_format = \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream = sys.stdout, \n",
    "    format = log_format, \n",
    "    level = logging.INFO\n",
    ")\n",
    "\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data\n",
    "\n",
    "The data was taken from the [Twitter API](https://developer.twitter.com) and the newspaper where selected according to a list found in [Diarios de Perú](http://www.diariosdeperu.com.pe) as well as annecdotal experience. The journals to investigate:\n",
    "\n",
    "| Newspaper | Twitter handle |\n",
    "| ----------- | ----------- |\n",
    "| El Comercio | elcomercio_peru |\n",
    "| La República | larepublica_pe |\n",
    "| Perú 21 | peru21noticias |\n",
    "| Trome | tromepe |\n",
    "| Gestión | Gestionpe |\n",
    "| Diario Correo | diariocorreo |\n",
    "| Diario Expreso | ExpresoPeru |\n",
    "| Diario Ojo | diarioojo |\n",
    "| Diario El Peruano | DiarioElPeruano |\n",
    "| Diario La Razón | larazon_pe |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "BASE_DIR = os.environ.get(\"BASE_DIR\")\n",
    "BEARER_TOKEN = os.environ.get(\"BEARER_TOKEN\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the user ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "newspapers = [\n",
    "    \"elcomercio_peru\",\n",
    "    \"larepublica_pe\",\n",
    "    \"peru21noticias\",\n",
    "    \"tromepe\",\n",
    "    \"Gestionpe\",\n",
    "    \"diariocorreo\",\n",
    "    \"ExpresoPeru\",\n",
    "    \"diarioojo\",\n",
    "    \"DiarioElPeruano\",\n",
    "    \"larazon_pe\",\n",
    "    \"elbuho_pe\",\n",
    "    \"ensustrece\"\n",
    "]\n",
    "headers = {\"Authorization\": f\"Bearer {BEARER_TOKEN}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m     username_url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://api.twitter.com/2/users/by/username/\u001b[39m\u001b[39m{\u001b[39;00mnewspaper\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(username_url, headers\u001b[39m=\u001b[39mheaders)\n\u001b[0;32m----> 7\u001b[0m     newspapers_id[newspaper] \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39;49mjson()[\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      9\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mBASE_DIR\u001b[39m}\u001b[39;00m\u001b[39m/data/raw/newspapers_id.json\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m write_file:\n\u001b[1;32m     10\u001b[0m     json\u001b[39m.\u001b[39mdump(newspapers_id, write_file)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'data'"
     ]
    }
   ],
   "source": [
    "newspapers_id = {}\n",
    "\n",
    "for newspaper in newspapers:\n",
    "    username_url = f\"https://api.twitter.com/2/users/by/username/{newspaper}\"\n",
    "    response = requests.get(username_url, headers=headers)\n",
    "\n",
    "    newspapers_id[newspaper] = response.json()[\"data\"][\"id\"]\n",
    "\n",
    "with open(f'{BASE_DIR}/data/raw/newspapers_id.json', 'w') as write_file:\n",
    "    json.dump(newspapers_id, write_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting tweets from newspapers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{BASE_DIR}/data/raw/newspapers_id.json', 'r') as read_file:\n",
    "    newspapers_id = json.load(read_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm choosing start time and end time of tweets in order to be able to select tweets from different times and be actively collecting tweets as the months go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-09 16:09:02,184 - root - INFO - elcomercio_peru: status code 403\n",
      "2023-06-09 16:09:02,185 - root - INFO - elcomercio_peru: No data found!\n",
      "2023-06-09 16:09:02,480 - root - INFO - larepublica_pe: status code 403\n",
      "2023-06-09 16:09:02,480 - root - INFO - larepublica_pe: No data found!\n",
      "2023-06-09 16:09:02,753 - root - INFO - peru21noticias: status code 403\n",
      "2023-06-09 16:09:02,754 - root - INFO - peru21noticias: No data found!\n",
      "2023-06-09 16:09:03,055 - root - INFO - tromepe: status code 403\n",
      "2023-06-09 16:09:03,055 - root - INFO - tromepe: No data found!\n",
      "2023-06-09 16:09:03,342 - root - INFO - Gestionpe: status code 403\n",
      "2023-06-09 16:09:03,343 - root - INFO - Gestionpe: No data found!\n",
      "2023-06-09 16:09:03,636 - root - INFO - diariocorreo: status code 403\n",
      "2023-06-09 16:09:03,637 - root - INFO - diariocorreo: No data found!\n",
      "2023-06-09 16:09:03,950 - root - INFO - ExpresoPeru: status code 403\n",
      "2023-06-09 16:09:03,951 - root - INFO - ExpresoPeru: No data found!\n",
      "2023-06-09 16:09:04,263 - root - INFO - diarioojo: status code 403\n",
      "2023-06-09 16:09:04,264 - root - INFO - diarioojo: No data found!\n",
      "2023-06-09 16:09:04,552 - root - INFO - DiarioElPeruano: status code 403\n",
      "2023-06-09 16:09:04,553 - root - INFO - DiarioElPeruano: No data found!\n",
      "2023-06-09 16:09:04,848 - root - INFO - larazon_pe: status code 403\n",
      "2023-06-09 16:09:04,849 - root - INFO - larazon_pe: No data found!\n",
      "2023-06-09 16:09:05,132 - root - INFO - elbuho_pe: status code 403\n",
      "2023-06-09 16:09:05,133 - root - INFO - elbuho_pe: No data found!\n",
      "2023-06-09 16:09:05,434 - root - INFO - ensustrece: status code 403\n",
      "2023-06-09 16:09:05,436 - root - INFO - ensustrece: No data found!\n"
     ]
    }
   ],
   "source": [
    "for newspaper, newspaper_id in newspapers_id.items():\n",
    "    query = {\n",
    "    \"max_results\": 100,\n",
    "    \"tweet.fields\": \"id,text,created_at,public_metrics,possibly_sensitive,referenced_tweets\",\n",
    "    \"start_time\": \"2023-05-22T00:00:00Z\",\n",
    "    \"end_time\": \"2023-05-29T00:00:00Z\",\n",
    "    }\n",
    "    payload = urlencode(query, safe=\",:\")\n",
    "\n",
    "    DATA_DIR = f\"{BASE_DIR}/data/raw\"\n",
    "    SAVED_DATE = datetime.strptime(query[\"end_time\"][0:10],\"%Y-%m-%d\")\n",
    "    \n",
    "    tweets_url = f\"https://api.twitter.com/2/users/{newspaper_id}/tweets\"\n",
    "    response = requests.get(tweets_url, headers=headers, params=payload)\n",
    "\n",
    "    logger.info(f\"{newspaper}: status code {response.status_code}\")\n",
    "\n",
    "    try:\n",
    "        response_data = response.json()[\"data\"] # List of tweets\n",
    "        response_meta = response.json()[\"meta\"]\n",
    "    except KeyError:\n",
    "        logger.info(f\"{newspaper}: No data found!\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        next_token = response_meta[\"next_token\"]\n",
    "        query[\"pagination_token\"] = next_token\n",
    "        payload = urlencode(query, safe=\",:\")\n",
    "    except KeyError:\n",
    "        logger.info(f\"{newspaper}: No MORE data found!\")\n",
    "\n",
    "        with open(f\"{DATA_DIR}/{SAVED_DATE.isocalendar().year}w{SAVED_DATE.isocalendar().week}_data_{newspaper}.json\", \"w\") as write_file:\n",
    "            json.dump({\"data\": response_data}, write_file)\n",
    "        logging.info(f\"{newspaper}: Saved! FIRST\")\n",
    "        continue\n",
    "\n",
    "    while True:\n",
    "        logger.info(f\"{newspaper}: New page\")\n",
    "        new_response = requests.get(tweets_url, headers=headers, params=payload)\n",
    "\n",
    "        try:\n",
    "            response_data += new_response.json()[\"data\"]\n",
    "            response_meta = new_response.json()[\"meta\"]\n",
    "\n",
    "            next_token = response_meta[\"next_token\"]\n",
    "            query[\"pagination_token\"] = next_token\n",
    "            payload = urlencode(query, safe=\",:\")\n",
    "        except KeyError:\n",
    "            logger.info(f\"{newspaper}: No MORE data found!\")\n",
    "            \n",
    "            with open(f\"{DATA_DIR}/{SAVED_DATE.isocalendar().year}w{SAVED_DATE.isocalendar().week}_data_{newspaper}.json\", \"w\") as write_file:\n",
    "                json.dump({\"data\": response_data}, write_file)\n",
    "                logging.info(f\"{newspaper}: Saved!\")\n",
    "            break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-newspapersAnalysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "aef0c461847f6a88eedb677fa00493df291ae4b245cdf057d207d54f564bf672"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
